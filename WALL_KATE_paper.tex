\documentclass[review]{siamart251216}

% 1. Preamble and packages
\usepackage{amsfonts, amsmath, amssymb, amsbsy}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{algorithmic}
\ifpdf%
  \DeclareGraphicsExtensions{.eps,.pdf,.png,.jpg}
\else
  \DeclareGraphicsExtensions{.eps}
\fi
\usepackage{amsopn}
\DeclareMathOperator{\diag}{diag}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{todonotes}
\usepackage{caption}
\usepackage{float}

\DeclareCaptionType{equ}[Equation][List of Equations]
%\captionsetup[equ]{labelformat=empty}

% 1.5 Macros 
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\bigOh}{\mathcal{O}}
\renewcommand{\vec}[1]{{{\mathbf{#1}}}}
\renewcommand{\t}{{\vec  t }}
\newcommand{\s}{{\vec  s}}
\renewcommand{\u}{{\vec  u }}
\renewcommand{\v}{{\vec  v }}
\newcommand{\f}{{\vec  f }}
\newcommand{\zero}{{\vec  0 }}
\newcommand{\A}{{A }}
\newcommand{\offd}{{B}}
\newcommand{\Hank}{{ H }}
\newcommand{\Toe}{{ L }}
\newcommand{\toe}{{ \ell }}
\newcommand{\DST}{{ S }}
\newcommand{\Tau}{\mathcal{T}}
\newcommand{\ptau}{{\bm{\tau}}}
\newcommand{\Sq}{{S_Q}} 



% 2. Paper title
\newcommand{\TheTitle}{%
  Reviving Circulant Preconditioners for Adaptive Mesh Refinement
}

% 2.5. Short title for running heads (if needed)
\newcommand{\TheShortTitle}{%
  \TheTitle
}

% 3. Student Name
\newcommand{\TheName}{%
 K. Wall
}

% 4. Student Address
\newcommand{\TheAddress}{%
  Tufts University,
  (\email{kate.wall@tufts.edu}, \url{https://katejeanw.github.io/}).
}

% 5. Acknowledge funding or other resources
\newcommand{\TheFunding}{%
  This work was funded by NSF\@.
}

% 6. Collaborators, such as advisor or research collaborators
\newcommand{\TheCollaborators}{%
  James Adler,
  Xiaozhe Hu,
  Misha Kilmer
}

% ---------------------------------------------
% ---------------------------------------------
\author{\TheName\thanks{\TheAddress}}
\title{{\TheTitle}\thanks{\TheFunding}}
\headers{\TheShortTitle}{\TheName}
\ifpdf%
\hypersetup{%
  pdftitle={\TheTitle},
  pdfauthor={\TheName}
}
\fi

\begin{document}

\maketitle

\begin{center}
In collaboration with:
  {\TheCollaborators}
\end{center}
\vspace{1cm}
% ---------------------------------------------
% ---------------------------------------------

\begin{abstract}
We present a preconditioner for solving fractional partial differential equations (PDEs) on an adaptive mesh. Adaptive refinement of the problem domain results in a stiffness matrix with Toeplitz blocks along the main diagonal, while the fractional PDE yields a dense stiffness matrix, where off-diagonal blocks are stored as low-rank approximations. Our preconditioner utilizes ideas from the circulant preconditioner of Chan and Strang [SIAM Journal on Scientific Computing, 1989], which takes advantage of the Toeplitz blocks on the diagonal and also accounts for the low-rank nature of the off-diagonal blocks. We demonstrate its effectiveness at accelerating convergence for our systems and emphasize its efficient application. This work presents theoretical results about the spectral clustering of the preconditioned system. In order to prove these results, special consideration is taken on how the low-rank blocks perturb the eigenvalues of the Toeplitz block-diagonal system. Numerical tests for various fractional orders are used to inspect any assumptions and validate our results.
\end{abstract}




\begin{keywords}
  % 7. Keywords that describe the paper
  Preconditioner, Adaptive Refinement, Toeplitz, Circulant, DST, DCT
\end{keywords}

	


\section{Introduction} 
% Availabilitty of Xiaozhe's code / stiffness matrices 

There has long been interest in solving Toeplitz linear systems efficiently. A matrix $A$ is called Toeplitz if $a_{ij} = a_{i-j}$, in other words, $A$ has constant diagonals. Arbitrary $n \times n$ matrices have up to $n^2$ unique entries and are solved directly by traditional techniques in $\bigOh(n^3)$ time. Since a Toeplitz matrix has just at most $2n-1$ unique entries, we may expect to be able to solve it in $\bigOh (n^2)$ time. This is indeed the case via techniques such as Levinson's algorithm \cite{Levinson}. Even this improvement, however, is infeasible for large systems. Instead we turn to iterative Krylov and multigrid methods. For these methods we can still take advantage of Toeplitz structure by using circulant preconditioners. 

A circulant matrix is a Toeplitz matrix, that additionally has the ``wrap-around'' property where the last entry each row is the first entry of the subsequent row. 

\begin{equ}[H]
	\begin{equation}
	\Toe = \begin{pmatrix}
		\toe_0  &  \toe_{-1} & \cdots & \toe_{-(n-1)} \\
		\toe_{1} & \toe_0 & \cdots & \toe_{-(n-2)} \\
		\vdots & \vdots & \ddots & \vdots \\
		\toe_{n-1} & \toe_{n-2} & \cdots & \toe_0 
	\end{pmatrix} 
	\hspace{.15 in}
	C = \begin{pmatrix}
		c_0 & c_1 & \cdots & c_{n-1} \\
		c_{n-1} & c_0 & \cdots & c_{n-2} \\
		\vdots & \vdots & \ddots & \vdots \\
		c_{1} & c_{2} & \cdots & c_0 
		\end{pmatrix}
\end{equation}
\caption{$\Toe$ is a Toeplitz matrix and $C$ is circulant.}
\end{equ}

Toeplitz matrices commonly arise in PDE discretization, signal processing, and control theory. Often the Toeplitz matrices are also symmetric positive definite (SPD). Given an SPD Toeplitz system $\Toe x = b$, the idea introduced by Strang and Chan is to use certain circulant preconditioners $C$ so that $C^{-1}Tx = C^{-1}b$ is solved in fewer iterations \cite{Chan_Strang}.

We leverage this idea to build a preconditioner for stiffness matrices generated from the adaptive finite element method (AFEM) for fractional PDEs. In this setting the problem is discretized on a nonuniform mesh, and the resulting stiffness matrix is dense. In the usual finite element method (FEM) setting an uniform mesh results in a Toeplitz stiffness matrix. In the adaptive setting, after an initial solve on a uniform grid, the error on each element is estimated and the elements with largest error are refined via bisection. It is often the case that neighboring elements are refined the same number of times. So although the mesh is not globally uniform, there are areas of local uniformity. To build an effective preconditioner, we will take advantage of these locally uniform areas and their corresponding Toeplitz blocks in the stiffness matrix. 
% application for fractional pdes
% similar systems result from mixed precision 
\todo[inline]{TODO: citations for facts about FEM and Toeplitz matrices}

Although dense, the stiffness matrix can be effectively stored as a hierarchical matrix ($\mathcal{H}$-matrix). Due to weaker interaction between elements that are further apart in the domain, off-diagonal blocks are well-suited for low rank approximation. (See \cite{Xiaozhe} for more stiffness matrix details.) The low rank representation makes for fast computations, but complicates both the implementation of the preconditioner and the spectral clustering of the preconditioned system. 

% These preconditioners can be thought of as coming from Kernels of displacement operators. Cite Kalaith and Chan-Yeung.

In this paper we investigate how to precondition such systems using circulant matrices. Our investigation is focused on $\mathcal{H}$-matrices as in \cite{Xiaozhe}, but the same methods could be used on any matrix with Toeplitz blocks on the diagonal. We prove the preconditioned system has eigenvalues clustered around 1 and demonstrate numerical results with superlinear convergence. 

We emphasize that our unique contributions are:
\begin{itemize}
	\item building circulant preconditioners for adaptive meshes 
	\item proving the preconditioned system has eigenvalues clustered around 1
	\item something else? numerical results? dealing with low-rank blocks?
\end{itemize}

% overview of subsequent sections?

\section{Background}
%% FEM and matrix structure 
% Classical results on the eigenvalue distribution of Toeplitz matrices (see, e.g., [GS84]) indicate that we cannot expect, in general, any clustering, and that the convergence of the method will be slow.
To understand the need for our preconditioner, we must give a bit more detail about AFEM. We restrict our attention to problems on a one-dimensional domain, $[a,b]$ with the discretization $a = x_0, x_1, \dots, x_n = b$. Often FEM is  done on a uniform mesh, that is each element $[x_i, x_{i+1}]$ is size $x_{i+1}- x_i$ for all $0 \leq i \leq n-1$.

% brief proof uniform mesh gives toeplitz?
If the mesh is not fine enough to give the desired accuracy, one approach is to increase the number of elements, $n$. While this approach preserves uniformity, it usually requires recomputing the stiffness matrix entirely. Alternatively, if the level of refinement gives sufficiently small error for some parts of the domain, we can leave those unchanged and only refine in areas of larger error. This approach allows us to take advantage of computations that have already been performed, but the mesh is no longer uniform. 

In practice we find that adjacent elements are often refined to the same level, so a group of elements forms a locally uniform mesh. Since uniform meshes give rise to SPD Toeplitz systems, we can see that if we formed the stiffness matrix for just a locally uniform subdomain we would have an SPD Toeplitz matrix. So wherever there are adjacent elements of the same size we can find a corresponding Toeplitz block on the main diagonal of our stiffness matrix, $\A$. The size of this block depends on how many adjacent elements are the same size. We have also observed that the boundary of the domain almost always requires the greatest level of refinement. In general we have larger Toeplitz blocks from locally uniform subdomains in the middle of the matrix, and smaller blocks---or indeed $1 \times 1$ blocks---near the boundary. 

%% Toeplitz matrices and generating functions 
To build an effective preconditioner and investigate its properites, we have to take full advantage of these SPD Toeplitz blocks. Toeplitz matrices have many unique properties that give rise to efficient algorithms (see for example \cite{toeplitz}). For our purposes we focus on their connection to functions in the Wiener class. This will allow us to take our problem from matrix operator theory to function theory. Suppose we have a singly infinite, symmetric Toeplitz matrix
\begin{equation*}
	\Toe = \begin{pmatrix}
		\toe_0 & \toe_1 & \toe_2 & \\
		\toe_1 & \toe_0 & \toe_1 &  \ddots \\
		\toe_2 & \toe_1 & \toe_0 & \ddots  \\
		 & \ddots & \ddots & \ddots  \\
	\end{pmatrix}.
\end{equation*}
Assume $\sum_{k=-\infty}^{\infty} |\toe_k| <\infty$. Then the function $\toe(z) = \sum_{k=-\infty}^{\infty} \toe_k z^k$ is real, positive, and in the Wiener class for $|z| = 1$. It will be convenient to define the corresponding truncated function for a finite subsection of the infinite matrix: 
\begin{definition}
	The $m \times m$ finite subsection of the singly infinite matrix $T$ is denoted $T_m$ and defined as
	\begin{equation*}
				\Toe_m = \begin{pmatrix}
			\toe_0 & \toe_1 & \cdots & \toe_{m-1} \\
			\toe_1 & \toe_0 & \cdots & \toe_{m-2} \\
			\vdots & \vdots & \ddots & \vdots \\
			\toe_{m-1} & \toe_{m-2} & \cdots & \toe_0 
		\end{pmatrix}.
	\end{equation*}
	Similarly this matrix induces a function from the truncated series, $\toe_m(z) = \sum_{k=-(m-1)}^{m-1} \toe_k z^k$. 
\end{definition}

Previous work on Toeplitz systems offers a few options for circulant preconditioners. Although their construction differs, the spectrum of the preconditioned systems are asymptotically the same \cite{Chan_89}. We use the construction given by Bini and Benedetto \cite{Bini}. Given a symmetric Toeplitz matrix $\Toe$ we build a Hankel correction, $H$ where 
\begin{equation*}
	\Toe = \begin{pmatrix}
		\toe_0 & \toe_1 & \toe_2 & \cdots &\toe_{n-2}& \toe_{n-1} \\
		\toe_1 & \toe_0 & \toe_1 &  \cdots &\toe_{n-3}& \toe_{n-2} \\
		\toe_2 & \toe_1 & \toe_0 & \cdots &\toe_{n-4}& \toe_{n-3} \\
		\vdots & \vdots & \vdots & \ddots&\vdots & \vdots \\
		\toe_{n-2} & \toe_{n-3} & \toe_{n-4} & \cdots &\toe_0 & \toe_1 \\
		\toe_{n-1} & \toe_{n-2} & \toe_{n-3} & \cdots & \toe_1 & \toe_0 
	\end{pmatrix}
	\Hank = \begin{pmatrix}
		\toe_2 & \toe_3 & \toe_4 & \cdots & \toe_{n-1} & 0 & 0 \\
		\toe_3 & \toe_4 & \toe_5 & \cdots  & 0 & 0 & 0 \\
		\toe_4 & \toe_5 & \toe_6 & \cdots & 0 & 0 & \toe_{n-1} \\
		\vdots & \vdots & \vdots & \cdots & \vdots & \vdots & \vdots \\
		0 & 0 & 0 & \cdots & \toe_5 & \toe_4 & \toe_3 \\
		0 & 0 & \toe_{n-1} & \cdots & \toe_4 & \toe_3 & \toe_2
	\end{pmatrix}.
\end{equation*}

Then Bini and Benedetto's preconditioner is defined as $\ptau := \Toe - H$. $\Hank$ is a Hankel matrix, that is it is constant on the antidiagonals. Both $\Toe$ and $\Hank$ are symmetric, so they can be completely represented by the at most $n$ unique values: the diagonals of $\Toe: \toe_0, \toe_1, \dots, \toe_{n-1}$, and the antidiagonals of $\Hank: \toe_2, \toe_3, \dots, \toe_{n-1},0,0 $. 
	
We summarize the important properties of $\ptau$ (see \cite{Bini} for details):
\begin{itemize}
%	\item $\ptau$ is a circulant matrix.
	\item $\ptau$ is diagonalized by the type-I discrete sine transform (DST) matrix, $S$. We write $\ptau = S \Lambda S^{-1} = S \Lambda S$.  
	\item $\ptau$ can be applied in $n \log n$ time .
	\item The $k$-th eigenvalue of $\ptau$ is proportional to the $k$-th entry of $S\toe_1$ where $\toe_1$ is the first column of $\Toe$. Specifically, define  $c_k := \sqrt{\frac{n+1}{2}} \frac{1}{\sin(\frac{\pi k }{n+1})}$, then 
	\begin{equation} \label{eigs} \lambda_k(\ptau) = c_k [S\toe_1]_k \end{equation}.
	\item Each eigenvalue of $\ptau$ can be written as the truncated function $t_m$ evaluated somewhere on the unit circle, IE $\lambda_k(\ptau) = \toe_m(z_k)$ where $|z_k| = 1$.
\end{itemize}
These preconditioner can also be thought of as coming from the kernel of a displacement operator. This framework is useful for generating yet other circulant preconditioners, see \cite{Kailath}. 
% boundaries are the problem always (not infinite, block division)}
% Hankel matrices
% circulant and DFT/FFT (displacement kernel's)
% discrete convolution for exact solution and the boundary problem 

\section{Our Preconditioner}
In this section we set forth the properties we require from a preconditioner, how we use $\ptau$ in building our preconditioner, and how to build and apply our preconditioner efficiently.

A good preconditioner for an iterative method must in general decrease the total number of iterations, without increasing the cost of a single iteration. We borrow Kailath's \cite{Kailath} criteria for preconditioners, though similar criteria has been established in literature (for example \cite{Bini}). 
\begin{enumerate}
	\item Complexity of constructing  applying $\ptau$ should be $\bigOh (m \log m)$.
	\item A linear system with $\ptau$ should be solved in $ \bigOh (m \log m)$ operations.
	\item The spectrum of $\ptau^{-1} \A$ should be clustered around 1
\end{enumerate}

How tightly the eigenvalues cluster around 1 will determine the speed of convergence. In proving results about the clustering we will refer to the infinite matrix framework established previously. First we summarize results established previously about the spectral clustering of $\ptau$ applied to a single Toeplitz block $\Toe$. This result will then be used as a lemma in proving the spectral clustering for our stiffness matrix from the adaptive mesh. We will show that for $m$ large enough 

We make this last point more precise:
\begin{definition}[Eigenvalue Clustering] 
	For any $\varepsilon > 0$ we say the eigenvalues of a matrix $C^{-1} \Toe_m$ are clustered around a real number $\rho$ if there exists $N_1$ and $N_2$ such that for all $m > N_1$ there are at most $N_2$ eigenvalues of $C^{-1} \Toe_m$ that do not lie within $ [\rho-\varepsilon, \rho+\varepsilon]$.
\end{definition}

%The use of circulant preconditioners for Toeplitz systems originates from \cite{Chan_Strang}. Kailath showed how both Strang and Chan type preconditioners come from the kernel of displacement operators, they further detail eight specific preconditioners for each form of the discrete sine and cosine transforms. \todo[inline]{apply in n log n time}
%
%In our numerical results we use type TODO, discussed in \cite{Bini}. Though the results could be generalized to all eight forms. An important fact that we will use later:
%\todo[inline]{eigenvalues as points on function }

Assume our stiffness matrix has $k$ Toeplitz blocks $\Toe_1, \Toe_2, \dots, \Toe_k$ of respective sizes $m_1, m_2, \dots, m_k$. Assume additionally these are ordered as they appear along the main diagonal. Each block can be thought of as a singly infinite matrix with a corresponding generating function, $\toe_1(z), \toe_2(z), \dots \toe_k(z)$. Assume that for all $1 \leq i \leq k, \toe_i(z)  \sum_{j=-\infty}^{\infty} |t_j| <\infty$ and $\toe_i(z) > 0$ for $z$ on the unit circle. 
% Common assumptions for proof on one block 

We can now construct a preconditioner, $\Tau$ for the adaptive system. To explicitly construct $\Tau$ we can calculate the Hankel correction $\Hank_i$ for every Toeplitz block, $\Toe_i$. Then we define $\ptau_i = \Toe_i - \Hank_i$,  resulting in $k$ matrices $\ptau_1, \ptau_2, \dots, \ptau_k$. Finally we assemble $\Tau$ as the block diagonal matrix with $\ptau_i$ as the $i$-th block. 
$$\Tau = \begin{pmatrix}
		\ptau_1 &&& \\ & \ptau_2 && \\ && \ddots & \\ &&& \ptau_m
	\end{pmatrix}.$$

This explicit construction is not the most efficient, instead we can apply $\Tau^{-1}$ implicitly in $m \log m $ time where $m = \sum_{j=1}^k m_j$. 

Let $S_{m_i}$ denote the type-I DST matrix of size $m_i$.  Using equation \ref{eigs} to calculate the eigenvalues of $\ptau_i$, we can write $\ptau_i =  S_{m_i} \Lambda_i S_{m_i}$. 
\begin{equation*}
	\Tau^{-1} = 
	\begin{pmatrix}
		S_{m_1} &&& \\ & S_{m_2} && \\ && \ddots & \\ &&& S_{m_k}
	\end{pmatrix}
	\begin{pmatrix}
		\Lambda_1^{-1} &&& \\ & \Lambda_2^{-1} && \\ && \ddots & \\ &&& \Lambda_m^{-1}
	\end{pmatrix}
	\begin{pmatrix}
		S_{m_1} &&& \\ & S_{m_2} && \\ && \ddots & \\ &&& S_{m_k}
		\end{pmatrix}.
\end{equation*}
Multiplication of the $m \times m$ matrices that consist of the DST blocks can be done via the fast Fourier transform in $m \log m $ time. The eigenvalue matrix is diagonal and can be inverted in $m$ operations. The scaling done by this diagonal matrix could in the worst case cost $\bigOh(m^2)$, but as we will see in the next section, we need not scale our entire stiffness matrix. \todo[inline]{verify this is true...}
This establishes criteria 1 and 2 for $\Tau$. 

\todo[inline]{Bullet point properties of $\Tau$}


\section{Theoretical Results} 
% notation table ?
% 1x1 blocks consideration 
% Story: we know how this works on one block, what about block diagonal, what about with things happening outside of block diagonal? 
In this section we will prove that the spectral clustering in criteria 3 holds for $\Tau$. Proofs for clustering of circulant preconditioned problems int his area tend to follow a similar structure. First it's shown that the preconditioned system having a spectrum clustered around 1 is equivalent to the clustering of a related system around 0. This related system is then split into the sum of a ``low-rank'' term and a ``small-norm'' term. The small-norm term has spectrum clustered around 0, forcing the clustering of the entire system around 0 with the number of outliers bounded by the rank of the low-rank term.

We present our version of the spectral clustering proof for the case of $\ptau$ acting on a single Toeplitz block. This result is then used to help us prove the clustering of our preconditioner on the adaptive grid, $\Tau^{-1} \A$. 
\todo[inline]{hyphenation?}

% \begin{corollary}	The matrices $\Sq(A_m)$ and $\Sq(A_m)^{-1}$ are positive definite.\end{corollary}

%To effectively utilize the small-norm and low-rank splitting we will use the eigenvalue min-max theorem, and its corollary Weyl's inequality. Versions are both that will be useful to us are restated here. 
%\begin{theorem}[Min-max Theorem]
%	Let $A \in \R^{n \times n}$ be symmetric with ordered spectrum $\lambda_1 \geq \cdots \geq \lambda_n$. Then 
%	\begin{align*}
%		\lambda_k &= \mathop{\max_{V \subset \R^{n \times n}}}_{\dim (V) = k} \ \mathop{\min_{x \in V}}_{x \ne \zero} \frac{(Ax,x)}{(x,x)} .
%	\end{align*} 
%\end{theorem}
For convenience we restate Weyl's inequality, which will be useful to us later. 
\begin{lemma}[Weyl's Inequality] \\ Let $M$ and $E$ be Hermitian $m \times m$ matrices. Then for $A := M + E$ we have 
	$$ | \lambda_k(A) - \lambda_k(M) | \leq ||E||_2 \ , 1 \leq k \leq m.$$
	That is the eigenvalues of $A$ are at most $||E||_2$ away from the eigenvalues of $M$. 
\end{lemma}

\begin{lemma}[Eigenvalue Clustering on a Toeplitz Block]
	Let $\Toe$ be a singly infinite, symmetric, Toeplitz matrix with diagonals $\toe_0, \toe_1, \dots$. Then $\Toe_m$ is its $m \times m$ truncation with diagonals $\toe_0, \toe_1, \dots, \toe_{m-1}$. Assume the generating function $\toe(z)$ is in the Wiener class and positive on the unit circle. Let $\ptau$ be the corresponding preconditioner, then for $m$ large enough the spectrum of $\ptau^{-1} \Toe$ is clustered around 1. 
\end{lemma}
\begin{proof}
	Since $\toe(z) > 0$ for $|z|=1$, compactness of the unit circle implies that for some $\varepsilon > 0$, $\toe(z) > 2 \varepsilon$ when $|z|= 1$. By the Wiener class assumption we can choose $N$ such that $\sum_{j=N}^{\infty} \toe_j z^j \leq \sum_{j=N}^{\infty} |\toe_j| < \varepsilon$. So for all $m \geq N$, 
	$$ 2 \varepsilon < \toe(z) = \toe_m(z) + \sum_{j=m}^{\infty} \toe_j z^j < \toe_m(z) + \varepsilon.$$
	Thus $\toe_m(z) > \varepsilon$ on the unit circle. 

Recall from the construction of $\ptau$, 
\begin{align}
	\ptau &= \Toe - \Hank \\
	\Toe &= \ptau + \Hank \\
	\implies  \ptau^{-1} \Toe &= I + \ptau^{-1} \Hank 
\end{align}
where $\Hank$ is the Hankel matrix with antidiagonals $\toe_2, \toe_3, \dots, \toe_{m-1},0,0$. Therefore it suffices to show that the eigenvalues of $\ptau^{-1} \Hank$ are clustered around 0. 

We now split $\Hank$ into a low-rank matrix $\Hank_{LR}$ that contains the antidiagonals $\toe_0, \dots, \toe_N$ and a small-norm matrix $\Hank_{SN}$ such that $\Hank = \Hank_{LR} + \Hank_{SN}$.  Let $s := \text{rank}(\Hank_{LR}) << m$. The small-norm descriptor is justified since $\Hank_{SN} = \Hank - \Hank_{LR}$ is a hermitian $m \times m$ matrix with at most two copies of $\toe_{N}, \dots, \toe_{m-1}$ in each row/column. Thus $||\Hank_{SN}||_2 = \sqrt{||\Hank_{SN}||_1 ||\Hank_{SN}||_{\infty}} = ||\Hank_{SN}||_1 < 2 \varepsilon$. Hence by Weyl's Inequality at least $m-s$ of the eigenvalues of $\Hank$ are clustered within $2 \varepsilon$ of zero. 

Now using the min-max theorem, 
\begin{align*}
	\lambda_k ( \ptau^{-1} \Hank  ) &= \min_{\dim V=k} \max_{x \in V} \left(  \frac{(\Hank x,x)}{(\ptau x,x)}\right) \\
	&\leq   \min_{\dim V =k} \left[ \max_{x \in V} \left( \frac{(\Hank x,x)}{(x,x)} \right)  \max_{x \in V} \left( \frac{(x,x)}{(\ptau x,x)} \right) \right] \\
	&\leq \left[  \min_{\dim V =k}  \max_{x \in V} \left( \frac{(\Hank x,x)}{(x,x)} \right)  \right] \max_{x \in \R^n} \left( \frac{(x,x)}{(\ptau x,x)} \right)  \\
	&= \lambda_k (\Hank)   \max_{x \in \R^n} \left( \frac{(x,x)}{(\ptau x,x)} \right)  \leq \lambda_k (\Hank)  \frac{1}{\lambda_{\min}(\ptau )} \\
	 & = \lambda_k (\Hank)  \frac{1}{a_m(z_{\min})} \leq \lambda_k (\Hank)  \frac{1}{\varepsilon}.
\end{align*}
Thus the clustering of the spectrum of $\Hank$ around 0 implies the same for $\ptau^{-1} \Hank$. 
\end{proof}

\subsection{Full Matrix Proof}
\subsubsection{setup}
A single block preconditioner is $\tau$ the block diagonal preconditioner is $\Tau$.

On a single block we write $\tau = \A - \Hank$, but for the full adaptive matrix $\A$ includes off diagonal blocks. Denote the diagonal (Toeplitz blocks) as $\A_D$ and everything else $\A_E$ so that $$\A = \A_D + \A_E + \offd$$. And thus the splitting as in \cite{Kailath} is expressed $\A_D = \Tau + H$ and $\A = \A_E + \offd + \Tau + \Hank$. So
\begin{equation}\label{decomp}
	\Tau^{-1} \A = \Tau^{-1}(\Tau + \Hank + \A_E + \offd) = I +\Tau^{-1}\Hank + \Tau^{-1}\A_E  +\Tau^{-1}\offd
\end{equation}
\subsubsection{Proof}
It suffices to show that $\Tau^{-1}\Hank$, $\Tau^{-1}\offd$ and $\Tau^{-1}\A_E$ have spectra clustered around zero. 
First notice that $\Tau^{-1} \Hank$ is block diagonal and the spectrum of each block can be characterized using the former proof on each block. 

\todo[inline]{since we don't really choose block size in practice the actual block size dictates the size of $\varepsilon$. Over all the blocks we can take the max $\varepsilon$ for a uniform bound, but many will be clustered tighter than that. Supports argument that bigger Toeplitz blocks = better clustering}

Assume the off-diagonal-by-one blocks are low-rank. Let $C$ be such a block with dimensions $n_C \times n_C$ and rank $r_C << n_C$. Using the SVD we can split $C$ as $$ C = \left( \sum_{i=1}^{r_{C}} \sigma_i^{(C)} \u_i ^{(C)}\v_i^{(C)}* \right) + \left(  \sum_{i=r_{C}+1}^{n_{C}} \sigma_i^{(C)} \u_i^{(C)} \v_i^{(C)}*  \right). $$ 
With a slight abuse of notation, we can embed this decomposition in the appropriate ``off-diagonal'' position of an $m \times m$ matrix. Doing this for all such off-diagonal blocks we write
\begin{align*}
	B &= \sum_{C \in \text{off-diag}} \left[ \left( \sum_{i=1}^{r_{C}} \sigma_i^{(C)} \u_i ^{(C)}\v_i^{(C)}* \right) + \left(  \sum_{i=r_{C}+1}^{n_{C}} \sigma_i^{(C)} \u_i^{(C)} \v_i^{(C)}*  \right) \right]  \\
	&= \left( \sum_{i=1}^{r_{\offd}} \sigma_i \u_i \v_i* \right) + \left(  \sum_{i=r_{\offd}+1}^{n_{\offd}} \sigma_i \u_i \v_i*  \right) 
\end{align*}
where $r_{\offd} = \max_{C \in \text{off-diag}} r_C$.

We additionally split $\Hank$ by separating the anti-diagonals with coefficients $a_0, \dots, a_N$ and the anti-diagonals comprising of $a_{N+1}, \dots a_m$. So we have two splittings, 
\begin{align*} 
	\offd &= \left( \sum_{i=1}^{r_{\offd}} \sigma_i \u_i \v_i* \right) + \left(  \sum_{i=r_{\offd}+1}^{n_{\offd}} \sigma_i \u_i \v_i*  \right) \\
	\Hank &= \Hank|_{a_{0},\dots,a_N} + \Hank|_{a_{N+1},\dots,a_m}.
\end{align*}
The first term in each sum can be thought of as our `low-rank' equivalent from before and similarly the second term is our `small-norm' summand. \todo[inline]{Bound on number of off diagonal blocks} 

Finally we can make the splitting $\A = \A_{SN} + \A_{LR}$ where
\begin{align*}
	\A_{SN} &= \Hank|_{a_{N+1},\dots,a_m} + \sum_{i=r_{\offd}+1}^{n_{\offd}} \sigma_i \u_i \v_i* +A_E \\
	\A_{LR} &= \Hank|_{a_{0},\dots,a_N} + \sum_{i=1}^{r_{\offd}} \sigma_i \u_i \v_i*.
\end{align*}
$\A_{LR}$ represent outliers, IE $s := \text{rank}(\A_{LR}) \leq N + r_{\offd}$ bounds the number of outliers. \todo[inline]{Is the N part of this bound true? 2N?} So the work is showing $|| \Tau^{-1}\A_{SN}||_2 \leq \varepsilon$. Define $\tilde{\offd} = \sum_{i=r_{\offd}+1}^{n_{\offd}} \sigma_i \u_i \v_i* $ and $\tilde{\Hank} = \Hank|_{a_{N+1},\dots,a_m}$, so that $\A_{SN} = \tilde{\Hank} + \tilde{\offd} + \A_E$. 
\begin{align*}
	|| \Tau^{-1}\A_{SN}||_2 &\leq ||\Tau^{-1} \tilde{\Hank}||_2 + ||\Tau^{-1} \tilde{\offd}||_2 +||\Tau^{-1} \A_E ||_2
\end{align*}
We can bound $||\Tau^{-1} \tilde{\Hank}||_2$ as in \cite{Kailath}. We can bound $||\Tau^{-1} \A_E||_2$ with Weyl's inequality:
\begin{equation*}
	||\Tau^{-1} \A_E||_2 \leq ||\Tau^{-1}||_2 ||\A_E||_2 =\sigma_{\max}(\Tau^{-1}) \sigma_{\max}(\A_E) = \frac{ \sigma_{\max}(\A_E)}{\lambda_{\min}(\Tau)}.
\end{equation*}
Finally we bound $||\Tau^{-1} \tilde{\offd}||_2$.
\begin{align*}
	\lambda_k ( \Tau^{-1} \tilde{\offd}) &= \min_{\dim V=k} \max_{x \in V} \left(  \frac{(\tilde{\offd}x,x)}{(\Tau x,x)}\right) \\
	&\leq   \min_{\dim V =k} \left[ \max_{x \in V} \left( \frac{(\tilde{\offd} x,x)}{(x,x)} \right)  \max_{x \in V} \left( \frac{(x,x)}{(\Tau x,x)} \right) \right] \\
	&\leq \left[  \min_{\dim V =k}  \max_{x \in V} \left( \frac{(\tilde{\offd} x,x)}{(x,x)} \right)  \right] \max_{x \in \R^n} \left( \frac{(x,x)}{(\Tau x,x)} \right)  \\
	&= \lambda_k(\tilde{\offd})   \max_{x \in \R^n} \left( \frac{(x,x)}{(\Tau x,x)} \right)  \\
	&\leq \lambda_k(\tilde{\offd})  \frac{1}{\lambda_{\min}(\Tau)} \\
	&= \lambda_k(\tilde{\offd})  \min_{n \in n_k} \min_{1 \leq i \leq n} \frac{\sin(\frac{\pi i}{n+1})}{\sum_{j=1}^n t_j \sin(\frac{\pi i j }{n+1})}
\end{align*}
Since $\tilde{\offd}$ made of blocks that have form $\sum_{i=r_{\offd}+1}^{n_{\offd}} \sigma_i \u_i \v_i*$ what can we say about $\lambda_k$?  

\begin{itemize}
	\item numerical test confirming off-diag low rank
	\item Explanation and tests showing off-off-diag are small norm 
	\item technically lots of $ 1 \times 1$ blocks at boundaries, these get jacobi inverse treatment so are clustered around 1 
	\item Comment - all problems come from boundaries
	\item Extend proof to different kinds of circulant preconditioner
\end{itemize}

\section{Numerical Results}
\begin{itemize}
	\item enough info to reproduce 
	\item Single block clustering
	\item Adaptive clustering (what happens to smallest eigenvalue?)
	\item behavior for different $\alpha$
	\item Verify assumptions from proof
	\item convergence of solving with PCG (superlinear convergence)
\end{itemize}

\section{Conclusion}
Future work: how to build adaptive mesh to increase block size, other circulant preconditioners, tensor preconditioners, higher dimension domain, mixed precision 

\bibliographystyle{siamplain}
\bibliography{references}

\end{document}
